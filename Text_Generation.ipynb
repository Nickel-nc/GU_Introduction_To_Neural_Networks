{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [TF LSTM Text Gen 'Alice in wonderland'](#baseline)\n",
    "* [Numpy LSTM implementation 'Shakespeare'](#numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LSTM Text Gen 'Alice in wonderland'\n",
    "<a class=\"anchor\" id=\"baseline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, random, math\n",
    "from collections import Counter\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "Генерация из посева: rge, ugly \n",
      "rge, ugly and alice and stoopere the tar said. 'i can't got bigger again.' she sat down and stood and stood an==================================================\n",
      "Итерация #: 1\n",
      "Генерация из посева: ey finishe\n",
      "ey finished their chocolates near her anemel, she heard somebody something hard hit her from here choure. 's a==================================================\n",
      "Итерация #: 2\n",
      "Генерация из посева:  he came i\n",
      " he came inside and ran up the stairs. 'this is very strange,' she thought. 'i'm not more than three meters hi==================================================\n",
      "Итерация #: 3\n",
      "Генерация из посева: en next?' \n",
      "en next?' she wondered. she looked at the table. 'why can't i get smaller. then i can get under the tree for a==================================================\n",
      "Итерация #: 4\n",
      "Генерация из посева: ' thought \n",
      "' thought alice. 'chocolates noisily. some of the country. 'please, why is your cat smiling?' 'because it's a ==================================================\n",
      "Итерация #: 5\n",
      "Генерация из посева: ry!' she s\n",
      "ry!' she said. 'that's strange!' thought alice. she stood up tall and looked at the bottle down quickly. 'perh==================================================\n",
      "Итерация #: 6\n",
      "Генерация из посева: , 'i cut s\n",
      ", 'i cut some more bread-and-butter.' but then she saw a box of chocolates, chocolates, chocolates, chocolates==================================================\n",
      "Итерация #: 7\n",
      "Генерация из посева:  from it. \n",
      " from it. 'i know a lot of the animals and birds wanted more. but the mouse. 'i'll speak to duchesses?' she wo==================================================\n",
      "Итерация #: 8\n",
      "Генерация из посева: r. she ate\n",
      "r. she ate some of the brown mushroom. 'eat from my mushroom and you'll come to a house with one very large ro==================================================\n",
      "Итерация #: 9\n",
      "Генерация из посева: 't get big\n",
      "'t get bigger again,' she thought. the baby started to get bigger again,' she thought. the baby started to get==================================================\n",
      "Итерация #: 10\n",
      "Генерация из посева: face is ch\n",
      "face is changing! oh! it's not difficult for you, but first, you tell me. who are you?' 'why do i have to get ==================================================\n",
      "Итерация #: 11\n",
      "Генерация из посева:  hungrily \n",
      " hungrily at the cat. 'that's strange!' she thought. 'i hope the march hare said quietly, 'don't be stupid! ho==================================================\n",
      "Итерация #: 12\n",
      "Генерация из посева: endly to m\n",
      "endly to me,' thought alice. she started to get bigger again. in a short time, she came to a pretty little hot==================================================\n",
      "Итерация #: 13\n",
      "Генерация из посева: ce answere\n",
      "ce answered. 'there isn't any,' said the mad hatter said. 'you see, i was right. butter isn't good for a watch==================================================\n",
      "Итерация #: 14\n",
      "Генерация из посева: think abou\n",
      "think about it.' 'bring the duchess shouted. the dodo answered. 'you stupid woman,' said alice. 'i want to go ==================================================\n",
      "Итерация #: 15\n",
      "Генерация из посева: ce thought\n",
      "ce thought for a minute. then she saw a little bottle. alice found a place and sat down. but she was too big f==================================================\n",
      "Итерация #: 16\n",
      "Генерация из посева: me with hi\n",
      "me with his finger in his hand on her arm. he said quietly. 'the duchess shouted. the dodo and stood round ali==================================================\n",
      "Итерация #: 17\n",
      "Генерация из посева: didn't fee\n",
      "didn't feel afraid. 'the queen said to the king. 'what was in the water too. chapter ten the end of the table.==================================================\n",
      "Итерация #: 18\n",
      "Генерация из посева:  sat on bi\n",
      " sat on big chairs above all the animals and birds in different times and stopped and looked at the tarts. 'i ==================================================\n",
      "Итерация #: 19\n",
      "Генерация из посева: shouted th\n",
      "shouted the queen shouted. the three gardeners but couldn't see the hole end?' she wondered. then she thought.==================================================\n",
      "Итерация #: 20\n",
      "Генерация из посева: the mad ha\n",
      "the mad hatter said. 'you see, i change all the animals and birds in the water was her tears. something here, ==================================================\n",
      "Итерация #: 21\n",
      "Генерация из посева: cut some m\n",
      "cut some more bread-and-butter.' 'but who will give us the chocolates near her. some were white, and - and-' t==================================================\n",
      "Итерация #: 22\n",
      "Генерация из посева:  told you,\n",
      " told you, i don't want to meet a strange little sound and she couldn't see the hole anywhere. 'how am i going==================================================\n",
      "Итерация #: 23\n",
      "Генерация из посева: et bigger \n",
      "et bigger again,' said the caterpillar. 'i'll try and tell you,' said the caterpillar. 'i'll try and tell you,==================================================\n",
      "Итерация #: 24\n",
      "Генерация из посева:  vanished,\n",
      " vanished, and there was a little bottle on it. 'that bottle was not on the table. 'we move from place to plac\n"
     ]
    }
   ],
   "source": [
    "# построчное чтение из примера с текстом \n",
    "with open(\"texts/alice_in_wonderland.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов \n",
    "SEQLEN, STEP = 10, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 512, 128\n",
    "NUM_ITERATIONS = 25 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 50\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION, verbose=0)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy RNN implementation with 'Shakespeare'\n",
    "<a class=\"anchor\" id=\"numpy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фреймворк, предложенный Э. Траском\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "\n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output    \n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "    \n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "    \n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "        \n",
    "    def step(self, zero=True):\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            \n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
    "\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "\n",
    "\n",
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "f = open('texts/William_Shakespeare_-_The_Sonnets.txt','r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   From fairest creatures we desire increase,\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=400):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if(t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(n=70, init_char='\\n').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()\n",
    "\n",
    "\n",
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "#         m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 211/214 - Loss:8.343153890725832-  e no un no un no un und un no und un un no no non she un un un un unu\n",
      " Iter:1 - Batch 211/214 - Loss:7.8851870596854585 e be be be be be be be und be und be be be be be but be be be be but \n",
      " Iter:2 - Batch 211/214 - Loss:7.5161906941238446 e be be the be be be be be be be ber be be be be be be be be be be uu\n",
      " Iter:3 - Batch 211/214 - Loss:7.2144886382374245 e be be be sut be be uud sut be be be be be be the be be be be sut be\n",
      " Iter:4 - Batch 211/214 - Loss:6.954829958453778  e be be be be be be be be be be beut stere be be us be beut be stere \n",
      " Iter:5 - Batch 211/214 - Loss:6.7059126890908075 e be stere be the be be be be be us be und be stere us be stere the b\n",
      " Iter:6 - Batch 211/214 - Loss:6.4640211709359905e be the be stere uut be us be stere the us us stere be und stere be \n",
      " Iter:7 - Batch 211/214 - Loss:6.239459756936543  e be the us stere stere us be the us us us be the stere the stere us \n",
      " Iter:8 - Batch 211/214 - Loss:6.020438549922309  e be the us us us us the stere the us us us us the us us us us the st\n",
      " Iter:9 - Batch 211/214 - Loss:5.805390544527284  e be the stere the us the us the stere the stere the stere the us us \n",
      " Iter:10 - Batch 211/214 - Loss:5.6056030731523015 u be the stere the stere the stere the stere the stere und be the us \n",
      " Iter:11 - Batch 211/214 - Loss:5.430643247254511  u be the us us us us the us us the us us us the stere the stere the s\n",
      " Iter:12 - Batch 211/214 - Loss:5.2787370267641475u us the be the us us beuse the stere the be stere the be stere the b\n",
      " Iter:13 - Batch 211/214 - Loss:5.1452073162184755 u stere the be uugh be und do stere the be und be uugh be stere the b\n",
      " Iter:14 - Batch 211/214 - Loss:5.0222676181700014 u be the be uugh be und be the be stere the be stere the be und be th\n",
      " Iter:15 - Batch 211/214 - Loss:4.904463852862473  u und un the be und und uugh uugh the be und be the be stere the be s\n",
      " Iter:16 - Batch 211/214 - Loss:4.7944433998303545u be und be the be stere the be stere the stere the be stere the be s\n",
      " Iter:17 - Batch 211/214 - Loss:4.6909459701140995 u be stere the be und un the be stere the und und un the und unusur s\n",
      " Iter:18 - Batch 211/214 - Loss:4.5925831936104916 u be stere the be stere the beauty beauty stere the be stere und ster\n",
      " Iter:19 - Batch 211/214 - Loss:4.498938410814811  u be stere the stere the und und un the stere the stere the stere the\n",
      " Iter:20 - Batch 211/214 - Loss:4.4097086668870375 u be stere the und do beauty beauty beauty beauty stere the stere the\n",
      " Iter:21 - Batch 211/214 - Loss:4.324087407768969-  u be stere the und do stere the stere the stere the stere the stere t\n",
      " Iter:22 - Batch 211/214 - Loss:4.2409315173322186  u be stere the stere the stere the stere the und und un the stere the\n",
      " Iter:23 - Batch 211/214 - Loss:4.159201620046633  u be stere the und do und do stere the stere the stere the stere und \n",
      " Iter:24 - Batch 211/214 - Loss:4.0789856186242037  u und stere the stere the stere the stere the stere the stere the ste\n"
     ]
    }
   ],
   "source": [
    "train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "u und be stere to but stere tur stere to but stere unot stere unot unot stere to but stere to but stere to but stere turught to but stere unot unot beauty but stere to beauty but stere to but stere to but stere to but stere to but stere to but unot beauty ut stere to but unot beauty but stere to but stere to but stere to but sterut stere unot stere unot unot stere to but stere to beauty but stere to but sterut stere to but stere to beauty but stere to but stere to but sterut stere to beauty but stere unot beauty ut beauty but stere to but stere to but stere to beauty but stere unot be stere to beauty to but stere to but stere to but stere to but unot stere to beauty but stere to but stere to but stere to but stere unot be unot stere unot ster the stere unot unot stere to but stere the stere to but stere to but stere to beauty but stere to beauty but stere to but stere to but stere the ut be stere unot stere to beauty sterut stere unot stere to but stere to but stere to but stere unot stere unot unou ut stere to beauty but stere to but sterut stere unot sterut unot unot be stere to but stere to beauty but stere to but unot stere unot beauty but stere to but ster the und be stere to but stere ture unot unot unot stere unot stere to but stere to but stere to beauty ture to but stere unot stere to but stere to beauty but stere to beauty but stere the stere turught tur stere unot stere unot be stere to but stere to but unou beauty ut beauty but stere to but stere to but stere to beauty but stere to but ster the stere unot unot stere to beauty but stere to but stere unot stere to but stere ture the ut be stere unot unot unot stere to but stere unot unot stere to beauty to but stere to but stere to but stere to but stere to beauty but unot unot unot stere to but stere ture to beauty but stere to beauty but sterut stere to beauty ture unot beused be stere to but stere to but stere to but stere ture to beauty but sterut stere unot stere to beauty ut beauty but stere to but \n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
