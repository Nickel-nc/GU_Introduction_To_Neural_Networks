{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [TF LSTM Text Gen 'Alice in wonderland'](#baseline)\n",
    "* [Numpy RNN, LSTM implementation 'Shakespeare' text](#numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LSTM Text Gen 'Alice in wonderland'\n",
    "<a class=\"anchor\" id=\"baseline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, random, math\n",
    "from collections import Counter\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "Генерация из посева: rge, ugly \n",
      "rge, ugly and alice and stoopere the tar said. 'i can't got bigger again.' she sat down and stood and stood an==================================================\n",
      "Итерация #: 1\n",
      "Генерация из посева: ey finishe\n",
      "ey finished their chocolates near her anemel, she heard somebody something hard hit her from here choure. 's a==================================================\n",
      "Итерация #: 2\n",
      "Генерация из посева:  he came i\n",
      " he came inside and ran up the stairs. 'this is very strange,' she thought. 'i'm not more than three meters hi==================================================\n",
      "Итерация #: 3\n",
      "Генерация из посева: en next?' \n",
      "en next?' she wondered. she looked at the table. 'why can't i get smaller. then i can get under the tree for a==================================================\n",
      "Итерация #: 4\n",
      "Генерация из посева: ' thought \n",
      "' thought alice. 'chocolates noisily. some of the country. 'please, why is your cat smiling?' 'because it's a ==================================================\n",
      "Итерация #: 5\n",
      "Генерация из посева: ry!' she s\n",
      "ry!' she said. 'that's strange!' thought alice. she stood up tall and looked at the bottle down quickly. 'perh==================================================\n",
      "Итерация #: 6\n",
      "Генерация из посева: , 'i cut s\n",
      ", 'i cut some more bread-and-butter.' but then she saw a box of chocolates, chocolates, chocolates, chocolates==================================================\n",
      "Итерация #: 7\n",
      "Генерация из посева:  from it. \n",
      " from it. 'i know a lot of the animals and birds wanted more. but the mouse. 'i'll speak to duchesses?' she wo==================================================\n",
      "Итерация #: 8\n",
      "Генерация из посева: r. she ate\n",
      "r. she ate some of the brown mushroom. 'eat from my mushroom and you'll come to a house with one very large ro==================================================\n",
      "Итерация #: 9\n",
      "Генерация из посева: 't get big\n",
      "'t get bigger again,' she thought. the baby started to get bigger again,' she thought. the baby started to get==================================================\n",
      "Итерация #: 10\n",
      "Генерация из посева: face is ch\n",
      "face is changing! oh! it's not difficult for you, but first, you tell me. who are you?' 'why do i have to get ==================================================\n",
      "Итерация #: 11\n",
      "Генерация из посева:  hungrily \n",
      " hungrily at the cat. 'that's strange!' she thought. 'i hope the march hare said quietly, 'don't be stupid! ho==================================================\n",
      "Итерация #: 12\n",
      "Генерация из посева: endly to m\n",
      "endly to me,' thought alice. she started to get bigger again. in a short time, she came to a pretty little hot==================================================\n",
      "Итерация #: 13\n",
      "Генерация из посева: ce answere\n",
      "ce answered. 'there isn't any,' said the mad hatter said. 'you see, i was right. butter isn't good for a watch==================================================\n",
      "Итерация #: 14\n",
      "Генерация из посева: think abou\n",
      "think about it.' 'bring the duchess shouted. the dodo answered. 'you stupid woman,' said alice. 'i want to go ==================================================\n",
      "Итерация #: 15\n",
      "Генерация из посева: ce thought\n",
      "ce thought for a minute. then she saw a little bottle. alice found a place and sat down. but she was too big f==================================================\n",
      "Итерация #: 16\n",
      "Генерация из посева: me with hi\n",
      "me with his finger in his hand on her arm. he said quietly. 'the duchess shouted. the dodo and stood round ali==================================================\n",
      "Итерация #: 17\n",
      "Генерация из посева: didn't fee\n",
      "didn't feel afraid. 'the queen said to the king. 'what was in the water too. chapter ten the end of the table.==================================================\n",
      "Итерация #: 18\n",
      "Генерация из посева:  sat on bi\n",
      " sat on big chairs above all the animals and birds in different times and stopped and looked at the tarts. 'i ==================================================\n",
      "Итерация #: 19\n",
      "Генерация из посева: shouted th\n",
      "shouted the queen shouted. the three gardeners but couldn't see the hole end?' she wondered. then she thought.==================================================\n",
      "Итерация #: 20\n",
      "Генерация из посева: the mad ha\n",
      "the mad hatter said. 'you see, i change all the animals and birds in the water was her tears. something here, ==================================================\n",
      "Итерация #: 21\n",
      "Генерация из посева: cut some m\n",
      "cut some more bread-and-butter.' 'but who will give us the chocolates near her. some were white, and - and-' t==================================================\n",
      "Итерация #: 22\n",
      "Генерация из посева:  told you,\n",
      " told you, i don't want to meet a strange little sound and she couldn't see the hole anywhere. 'how am i going==================================================\n",
      "Итерация #: 23\n",
      "Генерация из посева: et bigger \n",
      "et bigger again,' said the caterpillar. 'i'll try and tell you,' said the caterpillar. 'i'll try and tell you,==================================================\n",
      "Итерация #: 24\n",
      "Генерация из посева:  vanished,\n",
      " vanished, and there was a little bottle on it. 'that bottle was not on the table. 'we move from place to plac\n"
     ]
    }
   ],
   "source": [
    "# построчное чтение из примера с текстом \n",
    "with open(\"texts/alice_in_wonderland.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов \n",
    "SEQLEN, STEP = 10, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 512, 128\n",
    "NUM_ITERATIONS = 25 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 50\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION, verbose=0)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy RNN, LSTM implementation with 'Shakespeare'\n",
    "<a class=\"anchor\" id=\"numpy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фреймворк, предложенный Э. Траском\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,1000000000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                    print(self.id)\n",
    "                    print(self.creation_op)\n",
    "                    print(len(self.creators))\n",
    "                    for c in self.creators:\n",
    "                        print(c.creation_op)\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "    \n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "        \n",
    "    def step(self, zero=True):\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            \n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_bias = bias\n",
    "        \n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        if(self.use_bias):\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "        if(self.use_bias):        \n",
    "            self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if(self.use_bias):\n",
    "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
    "        return input.mm(self.weight)\n",
    "\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "\n",
    "\n",
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "    \n",
    "class LSTMCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "\n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)        \n",
    "        self.xc = Linear(n_inputs, n_hidden)        \n",
    "        \n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)        \n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()        \n",
    "        self.parameters += self.ho.get_parameters()        \n",
    "        self.parameters += self.hc.get_parameters()                \n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        prev_hidden = hidden[0]        \n",
    "        prev_cell = hidden[1]\n",
    "        \n",
    "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
    "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        \n",
    "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        \n",
    "        c = (f * prev_cell) + (i * g)\n",
    "\n",
    "        h = o * c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "        init_hidden.data[:,0] += 1\n",
    "        init_cell.data[:,0] += 1\n",
    "        return (init_hidden, init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN CELL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "f = open('texts/William_Shakespeare_-_The_Sonnets.txt','r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   From fairest creatures we desire increase,\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=400):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if(t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(n=70, init_char='\\n').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()\n",
    "\n",
    "\n",
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "#         m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 211/214 - Loss:8.343153890725832-  e no un no un no un und un no und un un no no non she un un un un unu\n",
      " Iter:1 - Batch 211/214 - Loss:7.8851870596854585 e be be be be be be be und be und be be be be be but be be be be but \n",
      " Iter:2 - Batch 211/214 - Loss:7.5161906941238446 e be be the be be be be be be be ber be be be be be be be be be be uu\n",
      " Iter:3 - Batch 211/214 - Loss:7.2144886382374245 e be be be sut be be uud sut be be be be be be the be be be be sut be\n",
      " Iter:4 - Batch 211/214 - Loss:6.954829958453778  e be be be be be be be be be be beut stere be be us be beut be stere \n",
      " Iter:5 - Batch 211/214 - Loss:6.7059126890908075 e be stere be the be be be be be us be und be stere us be stere the b\n",
      " Iter:6 - Batch 211/214 - Loss:6.4640211709359905e be the be stere uut be us be stere the us us stere be und stere be \n",
      " Iter:7 - Batch 211/214 - Loss:6.239459756936543  e be the us stere stere us be the us us us be the stere the stere us \n",
      " Iter:8 - Batch 211/214 - Loss:6.020438549922309  e be the us us us us the stere the us us us us the us us us us the st\n",
      " Iter:9 - Batch 211/214 - Loss:5.805390544527284  e be the stere the us the us the stere the stere the stere the us us \n",
      " Iter:10 - Batch 211/214 - Loss:5.6056030731523015 u be the stere the stere the stere the stere the stere und be the us \n",
      " Iter:11 - Batch 211/214 - Loss:5.430643247254511  u be the us us us us the us us the us us us the stere the stere the s\n",
      " Iter:12 - Batch 211/214 - Loss:5.2787370267641475u us the be the us us beuse the stere the be stere the be stere the b\n",
      " Iter:13 - Batch 211/214 - Loss:5.1452073162184755 u stere the be uugh be und do stere the be und be uugh be stere the b\n",
      " Iter:14 - Batch 211/214 - Loss:5.0222676181700014 u be the be uugh be und be the be stere the be stere the be und be th\n",
      " Iter:15 - Batch 211/214 - Loss:4.904463852862473  u und un the be und und uugh uugh the be und be the be stere the be s\n",
      " Iter:16 - Batch 211/214 - Loss:4.7944433998303545u be und be the be stere the be stere the stere the be stere the be s\n",
      " Iter:17 - Batch 211/214 - Loss:4.6909459701140995 u be stere the be und un the be stere the und und un the und unusur s\n",
      " Iter:18 - Batch 211/214 - Loss:4.5925831936104916 u be stere the be stere the beauty beauty stere the be stere und ster\n",
      " Iter:19 - Batch 211/214 - Loss:4.498938410814811  u be stere the stere the und und un the stere the stere the stere the\n",
      " Iter:20 - Batch 211/214 - Loss:4.4097086668870375 u be stere the und do beauty beauty beauty beauty stere the stere the\n",
      " Iter:21 - Batch 211/214 - Loss:4.324087407768969-  u be stere the und do stere the stere the stere the stere the stere t\n",
      " Iter:22 - Batch 211/214 - Loss:4.2409315173322186  u be stere the stere the stere the stere the und und un the stere the\n",
      " Iter:23 - Batch 211/214 - Loss:4.159201620046633  u be stere the und do und do stere the stere the stere the stere und \n",
      " Iter:24 - Batch 211/214 - Loss:4.0789856186242037  u und stere the stere the stere the stere the stere the stere the ste\n"
     ]
    }
   ],
   "source": [
    "train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "u und be stere to but stere tur stere to but stere unot stere unot unot stere to but stere to but stere to but stere turught to but stere unot unot beauty but stere to beauty but stere to but stere to but stere to but stere to but stere to but unot beauty ut stere to but unot beauty but stere to but stere to but stere to but sterut stere unot stere unot unot stere to but stere to beauty but stere to but sterut stere to but stere to beauty but stere to but stere to but sterut stere to beauty but stere unot beauty ut beauty but stere to but stere to but stere to beauty but stere unot be stere to beauty to but stere to but stere to but stere to but unot stere to beauty but stere to but stere to but stere to but stere unot be unot stere unot ster the stere unot unot stere to but stere the stere to but stere to but stere to beauty but stere to beauty but stere to but stere to but stere the ut be stere unot stere to beauty sterut stere unot stere to but stere to but stere to but stere unot stere unot unou ut stere to beauty but stere to but sterut stere unot sterut unot unot be stere to but stere to beauty but stere to but unot stere unot beauty but stere to but ster the und be stere to but stere ture unot unot unot stere unot stere to but stere to but stere to beauty ture to but stere unot stere to but stere to beauty but stere to beauty but stere the stere turught tur stere unot stere unot be stere to but stere to but unou beauty ut beauty but stere to but stere to but stere to beauty but stere to but ster the stere unot unot stere to beauty but stere to but stere unot stere to but stere ture the ut be stere unot unot unot stere to but stere unot unot stere to beauty to but stere to but stere to but stere to but stere to beauty but unot unot unot stere to but stere ture to beauty but stere to beauty but sterut stere to beauty ture unot beused be stere to but stere to but stere to but stere ture to beauty but sterut stere unot stere to beauty ut beauty but stere to but \n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM CELL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "f = open('texts/William_Shakespeare_-_The_Sonnets.txt','r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "model.w_ho.weight.data *= 0\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)\n",
    "min_loss = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2(iterations=400):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        min_loss = 1000\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "    #     batches_to_train = 32\n",
    "        for batch_i in range(batches_to_train):\n",
    "\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
    "\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
    "                batch_loss = criterion.forward(output, target)\n",
    "\n",
    "                if(t == 0):\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "\n",
    "            loss = losses[-1]\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data / bptt\n",
    "\n",
    "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
    "            if(epoch_loss < min_loss):\n",
    "                min_loss = epoch_loss\n",
    "                print()\n",
    "\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Alpha:\" + str(optim.alpha)[0:5]\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Min Loss:\" + str(min_loss)[0:5]\n",
    "            log += \" - Loss:\" + str(epoch_loss)\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(n=70, init_char='T').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 1 == 0):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        \n",
    "\n",
    "def generate_sample_2(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "#         m = (temp_dist > np.random.rand()).argmax() # sample from predictions\n",
    "        m = output.data.argmax() # take the max prediction\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iter:0 - Alpha:0.05 - Batch 1/274 - Min Loss:13.73 - Loss:13.733476742950888 - e                                                                     \n",
      " Iter:0 - Alpha:0.05 - Batch 2/274 - Min Loss:12.45 - Loss:12.45465200303372\n",
      " Iter:0 - Alpha:0.05 - Batch 102/274 - Min Loss:12.19 - Loss:12.217139572816114\n",
      " Iter:0 - Alpha:0.05 - Batch 103/274 - Min Loss:12.18 - Loss:12.185106907769212\n",
      " Iter:0 - Alpha:0.05 - Batch 104/274 - Min Loss:12.16 - Loss:12.164947753064684\n",
      " Iter:0 - Alpha:0.05 - Batch 105/274 - Min Loss:12.13 - Loss:12.138702315714331\n",
      " Iter:0 - Alpha:0.05 - Batch 106/274 - Min Loss:12.10 - Loss:12.109861730405465\n",
      " Iter:0 - Alpha:0.05 - Batch 107/274 - Min Loss:12.09 - Loss:12.095513713254318\n",
      " Iter:0 - Alpha:0.05 - Batch 108/274 - Min Loss:12.05 - Loss:12.054390246832655\n",
      " Iter:0 - Alpha:0.05 - Batch 109/274 - Min Loss:12.03 - Loss:12.036630843414233\n",
      " Iter:0 - Alpha:0.05 - Batch 110/274 - Min Loss:11.99 - Loss:11.993557845510182\n",
      " Iter:0 - Alpha:0.05 - Batch 111/274 - Min Loss:11.96 - Loss:11.96061603843132\n",
      " Iter:0 - Alpha:0.05 - Batch 112/274 - Min Loss:11.93 - Loss:11.93334108228492\n",
      " Iter:0 - Alpha:0.05 - Batch 113/274 - Min Loss:11.90 - Loss:11.900482569465572\n",
      " Iter:0 - Alpha:0.05 - Batch 114/274 - Min Loss:11.86 - Loss:11.865910840274175\n",
      " Iter:0 - Alpha:0.05 - Batch 120/274 - Min Loss:11.84 - Loss:11.845384524813557\n",
      " Iter:0 - Alpha:0.05 - Batch 121/274 - Min Loss:11.82 - Loss:11.82216934553853\n",
      " Iter:0 - Alpha:0.05 - Batch 122/274 - Min Loss:11.81 - Loss:11.815144682334209\n",
      " Iter:0 - Alpha:0.05 - Batch 123/274 - Min Loss:11.80 - Loss:11.809609502679484\n",
      " Iter:0 - Alpha:0.05 - Batch 124/274 - Min Loss:11.78 - Loss:11.782962049438206\n",
      " Iter:0 - Alpha:0.05 - Batch 125/274 - Min Loss:11.76 - Loss:11.762320366740404\n",
      " Iter:0 - Alpha:0.05 - Batch 126/274 - Min Loss:11.74 - Loss:11.745275285372973\n",
      " Iter:0 - Alpha:0.05 - Batch 128/274 - Min Loss:11.74 - Loss:11.741789872224768\n",
      " Iter:0 - Alpha:0.05 - Batch 129/274 - Min Loss:11.70 - Loss:11.707430810517401\n",
      " Iter:0 - Alpha:0.05 - Batch 130/274 - Min Loss:11.68 - Loss:11.684755502109187\n",
      " Iter:0 - Alpha:0.05 - Batch 131/274 - Min Loss:11.65 - Loss:11.657352802485466\n",
      " Iter:0 - Alpha:0.05 - Batch 133/274 - Min Loss:11.64 - Loss:11.652815142420522\n",
      " Iter:0 - Alpha:0.05 - Batch 134/274 - Min Loss:11.62 - Loss:11.62734883559263\n",
      " Iter:0 - Alpha:0.05 - Batch 135/274 - Min Loss:11.61 - Loss:11.619020347987556\n",
      " Iter:0 - Alpha:0.05 - Batch 137/274 - Min Loss:11.58 - Loss:11.593701107055178\n",
      " Iter:0 - Alpha:0.05 - Batch 139/274 - Min Loss:11.58 - Loss:11.591749424482222\n",
      " Iter:0 - Alpha:0.05 - Batch 140/274 - Min Loss:11.56 - Loss:11.565467356174436\n",
      " Iter:0 - Alpha:0.05 - Batch 141/274 - Min Loss:11.55 - Loss:11.556777084601908\n",
      " Iter:0 - Alpha:0.05 - Batch 142/274 - Min Loss:11.52 - Loss:11.526892505212817\n",
      " Iter:0 - Alpha:0.05 - Batch 143/274 - Min Loss:11.50 - Loss:11.506746577624648\n",
      " Iter:0 - Alpha:0.05 - Batch 144/274 - Min Loss:11.48 - Loss:11.480288885318995\n",
      " Iter:0 - Alpha:0.05 - Batch 145/274 - Min Loss:11.45 - Loss:11.455602251869736\n",
      " Iter:0 - Alpha:0.05 - Batch 146/274 - Min Loss:11.45 - Loss:11.45019976663545\n",
      " Iter:0 - Alpha:0.05 - Batch 147/274 - Min Loss:11.42 - Loss:11.425139334955027\n",
      " Iter:0 - Alpha:0.05 - Batch 148/274 - Min Loss:11.40 - Loss:11.409477553023054\n",
      " Iter:0 - Alpha:0.05 - Batch 149/274 - Min Loss:11.39 - Loss:11.393179301960458\n",
      " Iter:0 - Alpha:0.05 - Batch 150/274 - Min Loss:11.38 - Loss:11.383217125332468\n",
      " Iter:0 - Alpha:0.05 - Batch 151/274 - Min Loss:11.36 - Loss:11.369011636855223\n",
      " Iter:0 - Alpha:0.05 - Batch 152/274 - Min Loss:11.35 - Loss:11.359716795334165\n",
      " Iter:0 - Alpha:0.05 - Batch 154/274 - Min Loss:11.34 - Loss:11.342508074581586\n",
      " Iter:0 - Alpha:0.05 - Batch 155/274 - Min Loss:11.33 - Loss:11.334099426668688\n",
      " Iter:0 - Alpha:0.05 - Batch 156/274 - Min Loss:11.30 - Loss:11.303795189238661\n",
      " Iter:0 - Alpha:0.05 - Batch 157/274 - Min Loss:11.27 - Loss:11.279945476007725\n",
      " Iter:0 - Alpha:0.05 - Batch 173/274 - Min Loss:11.25 - Loss:11.255942389625757\n",
      " Iter:0 - Alpha:0.05 - Batch 174/274 - Min Loss:11.24 - Loss:11.240840094434347\n",
      " Iter:0 - Alpha:0.05 - Batch 177/274 - Min Loss:11.22 - Loss:11.223417324727283\n",
      " Iter:0 - Alpha:0.05 - Batch 178/274 - Min Loss:11.21 - Loss:11.215278550523411\n",
      " Iter:0 - Alpha:0.05 - Batch 179/274 - Min Loss:11.21 - Loss:11.212100230409975\n",
      " Iter:0 - Alpha:0.05 - Batch 180/274 - Min Loss:11.19 - Loss:11.192799426331105\n",
      " Iter:0 - Alpha:0.05 - Batch 181/274 - Min Loss:11.17 - Loss:11.170008507806129\n",
      " Iter:0 - Alpha:0.05 - Batch 182/274 - Min Loss:11.15 - Loss:11.155023289293004\n",
      " Iter:0 - Alpha:0.05 - Batch 183/274 - Min Loss:11.15 - Loss:11.150400334517688\n",
      " Iter:0 - Alpha:0.05 - Batch 184/274 - Min Loss:11.13 - Loss:11.132282344688674\n",
      " Iter:0 - Alpha:0.05 - Batch 185/274 - Min Loss:11.11 - Loss:11.114568427157128\n",
      " Iter:0 - Alpha:0.05 - Batch 186/274 - Min Loss:11.10 - Loss:11.102685393495756\n",
      " Iter:0 - Alpha:0.05 - Batch 187/274 - Min Loss:11.08 - Loss:11.080233893368863\n",
      " Iter:0 - Alpha:0.05 - Batch 188/274 - Min Loss:11.06 - Loss:11.06646760234474\n",
      " Iter:0 - Alpha:0.05 - Batch 189/274 - Min Loss:11.05 - Loss:11.059138027969563\n",
      " Iter:0 - Alpha:0.05 - Batch 190/274 - Min Loss:11.04 - Loss:11.048810772534805\n",
      " Iter:0 - Alpha:0.05 - Batch 191/274 - Min Loss:11.04 - Loss:11.040323707495595\n",
      " Iter:0 - Alpha:0.05 - Batch 192/274 - Min Loss:11.03 - Loss:11.032071505905666\n",
      " Iter:0 - Alpha:0.05 - Batch 193/274 - Min Loss:11.01 - Loss:11.01668309628398\n",
      " Iter:0 - Alpha:0.05 - Batch 194/274 - Min Loss:11.00 - Loss:11.003458398698534\n",
      " Iter:0 - Alpha:0.05 - Batch 198/274 - Min Loss:10.99 - Loss:10.994959197267875\n",
      " Iter:0 - Alpha:0.05 - Batch 199/274 - Min Loss:10.98 - Loss:10.982868064194806\n",
      " Iter:0 - Alpha:0.05 - Batch 200/274 - Min Loss:10.97 - Loss:10.972749050519116\n",
      " Iter:0 - Alpha:0.05 - Batch 201/274 - Min Loss:10.95 - Loss:10.95617390397547\n",
      " Iter:0 - Alpha:0.05 - Batch 202/274 - Min Loss:10.94 - Loss:10.94930474743105\n",
      " Iter:0 - Alpha:0.05 - Batch 203/274 - Min Loss:10.93 - Loss:10.935833360993858\n",
      " Iter:0 - Alpha:0.05 - Batch 204/274 - Min Loss:10.92 - Loss:10.929429791114872\n",
      " Iter:0 - Alpha:0.05 - Batch 205/274 - Min Loss:10.92 - Loss:10.921800272115927\n",
      " Iter:0 - Alpha:0.05 - Batch 206/274 - Min Loss:10.90 - Loss:10.907142302318086\n",
      " Iter:0 - Alpha:0.05 - Batch 207/274 - Min Loss:10.89 - Loss:10.898634982371037\n",
      " Iter:0 - Alpha:0.05 - Batch 208/274 - Min Loss:10.88 - Loss:10.881314464661553\n",
      " Iter:0 - Alpha:0.05 - Batch 209/274 - Min Loss:10.87 - Loss:10.874221077237351\n",
      " Iter:0 - Alpha:0.05 - Batch 210/274 - Min Loss:10.85 - Loss:10.857763326872623\n",
      " Iter:0 - Alpha:0.05 - Batch 212/274 - Min Loss:10.85 - Loss:10.852252986371704\n",
      " Iter:0 - Alpha:0.05 - Batch 213/274 - Min Loss:10.84 - Loss:10.841402205859529\n",
      " Iter:0 - Alpha:0.05 - Batch 214/274 - Min Loss:10.83 - Loss:10.837795327304635\n",
      " Iter:0 - Alpha:0.05 - Batch 215/274 - Min Loss:10.82 - Loss:10.828884204216415\n",
      " Iter:0 - Alpha:0.05 - Batch 216/274 - Min Loss:10.81 - Loss:10.816942841251699\n",
      " Iter:0 - Alpha:0.05 - Batch 218/274 - Min Loss:10.80 - Loss:10.808544232827613\n",
      " Iter:0 - Alpha:0.05 - Batch 219/274 - Min Loss:10.79 - Loss:10.797266761695653\n",
      " Iter:0 - Alpha:0.05 - Batch 220/274 - Min Loss:10.79 - Loss:10.795754045312993\n",
      " Iter:0 - Alpha:0.05 - Batch 221/274 - Min Loss:10.79 - Loss:10.792727310520156\n",
      " Iter:0 - Alpha:0.05 - Batch 222/274 - Min Loss:10.78 - Loss:10.785421704842612\n",
      " Iter:0 - Alpha:0.05 - Batch 223/274 - Min Loss:10.78 - Loss:10.780141111040237\n",
      " Iter:0 - Alpha:0.05 - Batch 224/274 - Min Loss:10.76 - Loss:10.769040080583373\n",
      " Iter:0 - Alpha:0.05 - Batch 225/274 - Min Loss:10.76 - Loss:10.760129342068531\n",
      " Iter:0 - Alpha:0.05 - Batch 227/274 - Min Loss:10.75 - Loss:10.75873044016284\n",
      " Iter:0 - Alpha:0.05 - Batch 228/274 - Min Loss:10.74 - Loss:10.749537279879714\n",
      " Iter:0 - Alpha:0.05 - Batch 229/274 - Min Loss:10.74 - Loss:10.740780772175619\n",
      " Iter:0 - Alpha:0.05 - Batch 235/274 - Min Loss:10.73 - Loss:10.737939949836216\n",
      " Iter:0 - Alpha:0.05 - Batch 236/274 - Min Loss:10.71 - Loss:10.71888733686384\n",
      " Iter:0 - Alpha:0.05 - Batch 237/274 - Min Loss:10.70 - Loss:10.704321826301024\n",
      " Iter:0 - Alpha:0.05 - Batch 238/274 - Min Loss:10.68 - Loss:10.688582444007425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Alpha:0.05 - Batch 239/274 - Min Loss:10.67 - Loss:10.679520319817016\n",
      " Iter:0 - Alpha:0.05 - Batch 240/274 - Min Loss:10.66 - Loss:10.666940797095844\n",
      " Iter:0 - Alpha:0.05 - Batch 241/274 - Min Loss:10.65 - Loss:10.652342561242117\n",
      " Iter:0 - Alpha:0.05 - Batch 242/274 - Min Loss:10.64 - Loss:10.643932457483293\n",
      " Iter:0 - Alpha:0.05 - Batch 243/274 - Min Loss:10.63 - Loss:10.634882175613182\n",
      " Iter:0 - Alpha:0.05 - Batch 245/274 - Min Loss:10.61 - Loss:10.619405658905222\n",
      " Iter:0 - Alpha:0.05 - Batch 246/274 - Min Loss:10.61 - Loss:10.610976486612602\n",
      " Iter:0 - Alpha:0.05 - Batch 247/274 - Min Loss:10.60 - Loss:10.605087707397498\n",
      " Iter:0 - Alpha:0.05 - Batch 248/274 - Min Loss:10.58 - Loss:10.589531215917402\n",
      " Iter:0 - Alpha:0.05 - Batch 249/274 - Min Loss:10.58 - Loss:10.580835876246509\n",
      " Iter:0 - Alpha:0.05 - Batch 250/274 - Min Loss:10.57 - Loss:10.570376681034794\n",
      " Iter:0 - Alpha:0.05 - Batch 251/274 - Min Loss:10.56 - Loss:10.560924056294068\n",
      " Iter:0 - Alpha:0.05 - Batch 252/274 - Min Loss:10.54 - Loss:10.546544648698834\n",
      " Iter:0 - Alpha:0.05 - Batch 253/274 - Min Loss:10.54 - Loss:10.5405546666778\n",
      " Iter:0 - Alpha:0.05 - Batch 254/274 - Min Loss:10.53 - Loss:10.532706028159362\n",
      " Iter:0 - Alpha:0.05 - Batch 256/274 - Min Loss:10.52 - Loss:10.526320045601288\n",
      " Iter:0 - Alpha:0.05 - Batch 257/274 - Min Loss:10.52 - Loss:10.520098420360343\n",
      " Iter:0 - Alpha:0.05 - Batch 258/274 - Min Loss:10.51 - Loss:10.517830555864052\n",
      " Iter:0 - Alpha:0.05 - Batch 259/274 - Min Loss:10.50 - Loss:10.505795362401656\n",
      " Iter:0 - Alpha:0.05 - Batch 260/274 - Min Loss:10.50 - Loss:10.501466763167247\n",
      " Iter:0 - Alpha:0.05 - Batch 261/274 - Min Loss:10.49 - Loss:10.491647777162928\n",
      " Iter:0 - Alpha:0.05 - Batch 262/274 - Min Loss:10.49 - Loss:10.49110203284451\n",
      " Iter:0 - Alpha:0.05 - Batch 263/274 - Min Loss:10.48 - Loss:10.486037190316107\n",
      " Iter:0 - Alpha:0.05 - Batch 264/274 - Min Loss:10.47 - Loss:10.479444513688351\n",
      " Iter:0 - Alpha:0.05 - Batch 265/274 - Min Loss:10.47 - Loss:10.470312461607193\n",
      " Iter:0 - Alpha:0.05 - Batch 266/274 - Min Loss:10.46 - Loss:10.462849557616547\n",
      " Iter:0 - Alpha:0.05 - Batch 267/274 - Min Loss:10.45 - Loss:10.452576719292562\n",
      " Iter:0 - Alpha:0.05 - Batch 268/274 - Min Loss:10.44 - Loss:10.444056707046741\n",
      " Iter:0 - Alpha:0.05 - Batch 269/274 - Min Loss:10.43 - Loss:10.438827581049233\n",
      " Iter:0 - Alpha:0.05 - Batch 270/274 - Min Loss:10.43 - Loss:10.43838672162157\n",
      " Iter:0 - Alpha:0.05 - Batch 271/274 - Min Loss:10.43 - Loss:10.430302625581724\n",
      " Iter:0 - Alpha:0.05 - Batch 272/274 - Min Loss:10.41 - Loss:10.414864131400796\n",
      " Iter:0 - Alpha:0.05 - Batch 273/274 - Min Loss:10.40 - Loss:10.407331845899773\n",
      " Iter:0 - Alpha:0.05 - Batch 274/274 - Min Loss:10.39 - Loss:10.39741306998527\n",
      " Iter:1 - Alpha:0.049 - Batch 1/274 - Min Loss:10.54 - Loss:10.543189671852861 - eren theren theuren theren theuren theren theren urend urend urend ofe\n",
      " Iter:1 - Alpha:0.049 - Batch 2/274 - Min Loss:9.294 - Loss:9.294060370485518\n",
      " Iter:1 - Alpha:0.049 - Batch 3/274 - Min Loss:8.996 - Loss:8.996499835013879\n",
      " Iter:1 - Alpha:0.049 - Batch 5/274 - Min Loss:8.928 - Loss:8.987473827911295\n",
      " Iter:1 - Alpha:0.049 - Batch 9/274 - Min Loss:8.714 - Loss:8.833821605655709\n",
      " Iter:1 - Alpha:0.049 - Batch 111/274 - Min Loss:8.705 - Loss:8.708097605405513\n",
      " Iter:1 - Alpha:0.049 - Batch 112/274 - Min Loss:8.701 - Loss:8.70134803951432\n",
      " Iter:1 - Alpha:0.049 - Batch 113/274 - Min Loss:8.697 - Loss:8.697427910448134\n",
      " Iter:1 - Alpha:0.049 - Batch 115/274 - Min Loss:8.690 - Loss:8.690200642116412\n",
      " Iter:1 - Alpha:0.049 - Batch 144/274 - Min Loss:8.688 - Loss:8.692170534249566\n",
      " Iter:1 - Alpha:0.049 - Batch 146/274 - Min Loss:8.680 - Loss:8.680773692607028\n",
      " Iter:1 - Alpha:0.049 - Batch 147/274 - Min Loss:8.674 - Loss:8.67488754927489\n",
      " Iter:1 - Alpha:0.049 - Batch 156/274 - Min Loss:8.669 - Loss:8.672245937812361\n",
      " Iter:1 - Alpha:0.049 - Batch 157/274 - Min Loss:8.664 - Loss:8.664786251029996\n",
      " Iter:1 - Alpha:0.049 - Batch 254/274 - Min Loss:8.655 - Loss:8.656673812190055\n",
      " Iter:1 - Alpha:0.049 - Batch 266/274 - Min Loss:8.649 - Loss:8.651938395820435\n",
      " Iter:1 - Alpha:0.049 - Batch 267/274 - Min Loss:8.647 - Loss:8.647288704628128\n",
      " Iter:1 - Alpha:0.049 - Batch 268/274 - Min Loss:8.643 - Loss:8.643779908036004\n",
      " Iter:1 - Alpha:0.049 - Batch 269/274 - Min Loss:8.641 - Loss:8.64130846105053\n",
      " Iter:1 - Alpha:0.049 - Batch 270/274 - Min Loss:8.641 - Loss:8.641165064286822\n",
      " Iter:1 - Alpha:0.049 - Batch 271/274 - Min Loss:8.638 - Loss:8.638738281931841\n",
      " Iter:1 - Alpha:0.049 - Batch 272/274 - Min Loss:8.630 - Loss:8.630635669018526\n",
      " Iter:1 - Alpha:0.049 - Batch 273/274 - Min Loss:8.628 - Loss:8.628842371389126\n",
      " Iter:1 - Alpha:0.049 - Batch 274/274 - Min Loss:8.623 - Loss:8.623816606916087\n",
      " Iter:2 - Alpha:0.049 - Batch 1/274 - Min Loss:9.617 - Loss:9.61753723792629 - urend urend ande urend andeurund ande the rend ande urend ande urend a\n",
      " Iter:2 - Alpha:0.049 - Batch 2/274 - Min Loss:8.381 - Loss:8.381179090450184\n",
      " Iter:2 - Alpha:0.049 - Batch 5/274 - Min Loss:8.348 - Loss:8.501899120807956\n",
      " Iter:2 - Alpha:0.049 - Batch 9/274 - Min Loss:8.257 - Loss:8.385169406362905\n",
      " Iter:2 - Alpha:0.049 - Batch 12/274 - Min Loss:8.222 - Loss:8.252477349478944\n",
      " Iter:2 - Alpha:0.049 - Batch 13/274 - Min Loss:8.186 - Loss:8.186359702144125\n",
      " Iter:2 - Alpha:0.049 - Batch 28/274 - Min Loss:8.170 - Loss:8.202647342184715\n",
      " Iter:2 - Alpha:0.049 - Batch 30/274 - Min Loss:8.164 - Loss:8.165218053680177\n",
      " Iter:2 - Alpha:0.049 - Batch 32/274 - Min Loss:8.133 - Loss:8.140662561718008\n",
      " Iter:2 - Alpha:0.049 - Batch 34/274 - Min Loss:8.085 - Loss:8.096549894940514\n",
      " Iter:2 - Alpha:0.049 - Batch 40/274 - Min Loss:8.067 - Loss:8.106022252851735\n",
      " Iter:2 - Alpha:0.049 - Batch 41/274 - Min Loss:8.064 - Loss:8.064554198984329\n",
      " Iter:2 - Alpha:0.049 - Batch 44/274 - Min Loss:8.047 - Loss:8.052208762708606\n",
      " Iter:2 - Alpha:0.049 - Batch 274/274 - Min Loss:8.029 - Loss:8.158516312795578\n",
      " Iter:3 - Alpha:0.048 - Batch 1/274 - Min Loss:9.363 - Loss:9.363858891472628 - he urend urend urend urend winde of the wurend urend urend urend winde\n",
      " Iter:3 - Alpha:0.048 - Batch 2/274 - Min Loss:8.220 - Loss:8.220328892961211\n",
      " Iter:3 - Alpha:0.048 - Batch 5/274 - Min Loss:8.220 - Loss:8.221130173035567\n",
      " Iter:3 - Alpha:0.048 - Batch 30/274 - Min Loss:7.982 - Loss:8.0104632023275525\n",
      " Iter:3 - Alpha:0.048 - Batch 31/274 - Min Loss:7.977 - Loss:7.9774379668039055\n",
      " Iter:3 - Alpha:0.048 - Batch 32/274 - Min Loss:7.975 - Loss:7.975929555541065\n",
      " Iter:3 - Alpha:0.048 - Batch 34/274 - Min Loss:7.913 - Loss:7.917734199592429\n",
      " Iter:3 - Alpha:0.048 - Batch 36/274 - Min Loss:7.876 - Loss:7.888448415443013\n",
      " Iter:3 - Alpha:0.048 - Batch 40/274 - Min Loss:7.854 - Loss:7.8747504557517205\n",
      " Iter:3 - Alpha:0.048 - Batch 41/274 - Min Loss:7.830 - Loss:7.830999185497421\n",
      " Iter:3 - Alpha:0.048 - Batch 43/274 - Min Loss:7.811 - Loss:7.812405269686921\n",
      " Iter:3 - Alpha:0.048 - Batch 44/274 - Min Loss:7.810 - Loss:7.810825730946121\n",
      " Iter:3 - Alpha:0.048 - Batch 70/274 - Min Loss:7.782 - Loss:7.7918688034606335\n",
      " Iter:3 - Alpha:0.048 - Batch 72/274 - Min Loss:7.770 - Loss:7.775639952468759\n",
      " Iter:3 - Alpha:0.048 - Batch 74/274 - Min Loss:7.751 - Loss:7.754843107662613\n",
      " Iter:3 - Alpha:0.048 - Batch 85/274 - Min Loss:7.750 - Loss:7.7540363414154125\n",
      " Iter:3 - Alpha:0.048 - Batch 86/274 - Min Loss:7.748 - Loss:7.748419010127321\n",
      " Iter:3 - Alpha:0.048 - Batch 87/274 - Min Loss:7.741 - Loss:7.741549664902408\n",
      " Iter:3 - Alpha:0.048 - Batch 274/274 - Min Loss:7.733 - Loss:7.8800976514498495\n",
      " Iter:4 - Alpha:0.048 - Batch 1/274 - Min Loss:9.550 - Loss:9.550821071923393 - hen when when whereurendeurender urendeurenderendeurenderundend urende\n",
      " Iter:4 - Alpha:0.048 - Batch 2/274 - Min Loss:8.534 - Loss:8.534137600456175\n",
      " Iter:4 - Alpha:0.048 - Batch 3/274 - Min Loss:8.410 - Loss:8.410049122972035\n",
      " Iter:4 - Alpha:0.048 - Batch 4/274 - Min Loss:8.368 - Loss:8.368179689841465\n",
      " Iter:4 - Alpha:0.048 - Batch 5/274 - Min Loss:8.271 - Loss:8.271511921581387\n",
      " Iter:4 - Alpha:0.048 - Batch 11/274 - Min Loss:8.016 - Loss:8.074098612675813\n",
      " Iter:4 - Alpha:0.048 - Batch 12/274 - Min Loss:8.009 - Loss:8.00972028230142\n",
      " Iter:4 - Alpha:0.048 - Batch 13/274 - Min Loss:7.907 - Loss:7.9070988515328695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:4 - Alpha:0.048 - Batch 14/274 - Min Loss:7.842 - Loss:7.842156029622567\n",
      " Iter:4 - Alpha:0.048 - Batch 26/274 - Min Loss:7.836 - Loss:7.9005907405119115\n",
      " Iter:4 - Alpha:0.048 - Batch 27/274 - Min Loss:7.814 - Loss:7.8142606079411605\n",
      " Iter:4 - Alpha:0.048 - Batch 28/274 - Min Loss:7.803 - Loss:7.803851913897792\n",
      " Iter:4 - Alpha:0.048 - Batch 30/274 - Min Loss:7.774 - Loss:7.783081373090212\n",
      " Iter:4 - Alpha:0.048 - Batch 31/274 - Min Loss:7.751 - Loss:7.7511643686779275\n",
      " Iter:4 - Alpha:0.048 - Batch 32/274 - Min Loss:7.746 - Loss:7.74622082198189\n",
      " Iter:4 - Alpha:0.048 - Batch 34/274 - Min Loss:7.682 - Loss:7.692329318358055\n",
      " Iter:4 - Alpha:0.048 - Batch 36/274 - Min Loss:7.652 - Loss:7.6634626956401695\n",
      " Iter:4 - Alpha:0.048 - Batch 38/274 - Min Loss:7.619 - Loss:7.620355296873931\n",
      " Iter:4 - Alpha:0.048 - Batch 40/274 - Min Loss:7.612 - Loss:7.6338482033981195\n",
      " Iter:4 - Alpha:0.048 - Batch 41/274 - Min Loss:7.590 - Loss:7.590360317690214\n",
      " Iter:4 - Alpha:0.048 - Batch 42/274 - Min Loss:7.570 - Loss:7.570278290214065\n",
      " Iter:4 - Alpha:0.048 - Batch 43/274 - Min Loss:7.567 - Loss:7.567593656200678\n",
      " Iter:4 - Alpha:0.048 - Batch 44/274 - Min Loss:7.561 - Loss:7.561759845810012\n",
      " Iter:4 - Alpha:0.048 - Batch 72/274 - Min Loss:7.520 - Loss:7.5254429187162964\n",
      " Iter:4 - Alpha:0.048 - Batch 74/274 - Min Loss:7.499 - Loss:7.499756064398842\n",
      " Iter:4 - Alpha:0.048 - Batch 84/274 - Min Loss:7.491 - Loss:7.5029216581217355\n",
      " Iter:4 - Alpha:0.048 - Batch 85/274 - Min Loss:7.490 - Loss:7.490891948975223\n",
      " Iter:4 - Alpha:0.048 - Batch 86/274 - Min Loss:7.485 - Loss:7.485568585838184\n",
      " Iter:4 - Alpha:0.048 - Batch 87/274 - Min Loss:7.477 - Loss:7.47708720716644\n",
      " Iter:4 - Alpha:0.048 - Batch 88/274 - Min Loss:7.467 - Loss:7.467434266036897\n",
      " Iter:4 - Alpha:0.048 - Batch 143/274 - Min Loss:7.464 - Loss:7.4721627704023095\n",
      " Iter:4 - Alpha:0.048 - Batch 144/274 - Min Loss:7.462 - Loss:7.462008318465837\n",
      " Iter:4 - Alpha:0.048 - Batch 156/274 - Min Loss:7.451 - Loss:7.4531768986303845\n",
      " Iter:4 - Alpha:0.048 - Batch 157/274 - Min Loss:7.443 - Loss:7.443034728923742\n",
      " Iter:4 - Alpha:0.048 - Batch 274/274 - Min Loss:7.430 - Loss:7.5437551898354875\n",
      " Iter:5 - Alpha:0.047 - Batch 1/274 - Min Loss:9.144 - Loss:9.144883910317947 - he when when wher winder the winder urendus urend urender uurend wiuth\n",
      " Iter:5 - Alpha:0.047 - Batch 5/274 - Min Loss:7.952 - Loss:8.0817388561863295\n",
      " Iter:5 - Alpha:0.047 - Batch 9/274 - Min Loss:7.815 - Loss:7.925446548551841\n",
      " Iter:5 - Alpha:0.047 - Batch 12/274 - Min Loss:7.766 - Loss:7.817079027884738\n",
      " Iter:5 - Alpha:0.047 - Batch 13/274 - Min Loss:7.738 - Loss:7.738328397103251\n",
      " Iter:5 - Alpha:0.047 - Batch 26/274 - Min Loss:7.698 - Loss:7.761220826579578\n",
      " Iter:5 - Alpha:0.047 - Batch 27/274 - Min Loss:7.682 - Loss:7.682580688996061\n",
      " Iter:5 - Alpha:0.047 - Batch 28/274 - Min Loss:7.670 - Loss:7.6706416782998454\n",
      " Iter:5 - Alpha:0.047 - Batch 30/274 - Min Loss:7.644 - Loss:7.666215866138898\n",
      " Iter:5 - Alpha:0.047 - Batch 31/274 - Min Loss:7.638 - Loss:7.638252163041508\n",
      " Iter:5 - Alpha:0.047 - Batch 32/274 - Min Loss:7.627 - Loss:7.627508382383533\n",
      " Iter:5 - Alpha:0.047 - Batch 34/274 - Min Loss:7.560 - Loss:7.5705551605941865\n",
      " Iter:5 - Alpha:0.047 - Batch 36/274 - Min Loss:7.530 - Loss:7.5398286063451945\n",
      " Iter:5 - Alpha:0.047 - Batch 37/274 - Min Loss:7.491 - Loss:7.491152898624734\n",
      " Iter:5 - Alpha:0.047 - Batch 38/274 - Min Loss:7.489 - Loss:7.489692977052202\n",
      " Iter:5 - Alpha:0.047 - Batch 40/274 - Min Loss:7.478 - Loss:7.498750658582502\n",
      " Iter:5 - Alpha:0.047 - Batch 41/274 - Min Loss:7.458 - Loss:7.458514555793143\n",
      " Iter:5 - Alpha:0.047 - Batch 42/274 - Min Loss:7.449 - Loss:7.4491844443351765\n",
      " Iter:5 - Alpha:0.047 - Batch 43/274 - Min Loss:7.446 - Loss:7.44616961513742\n",
      " Iter:5 - Alpha:0.047 - Batch 44/274 - Min Loss:7.435 - Loss:7.43531062693661\n",
      " Iter:5 - Alpha:0.047 - Batch 88/274 - Min Loss:7.377 - Loss:7.3800613303584355\n",
      " Iter:5 - Alpha:0.047 - Batch 89/274 - Min Loss:7.370 - Loss:7.370352404482556\n",
      " Iter:5 - Alpha:0.047 - Batch 157/274 - Min Loss:7.366 - Loss:7.3820364265791225\n",
      " Iter:5 - Alpha:0.047 - Batch 158/274 - Min Loss:7.365 - Loss:7.365703022745484\n",
      " Iter:5 - Alpha:0.047 - Batch 169/274 - Min Loss:7.356 - Loss:7.3586763887093465\n",
      " Iter:5 - Alpha:0.047 - Batch 170/274 - Min Loss:7.356 - Loss:7.356111644533745\n",
      " Iter:5 - Alpha:0.047 - Batch 274/274 - Min Loss:7.355 - Loss:7.4425700744783515\n",
      " Iter:6 - Alpha:0.047 - Batch 1/274 - Min Loss:8.828 - Loss:8.828705194413331 - he winde of the winde of thy urender thus winde of thu wheurender with\n",
      " Iter:6 - Alpha:0.047 - Batch 2/274 - Min Loss:7.524 - Loss:7.52434897983293\n",
      " Iter:6 - Alpha:0.047 - Batch 5/274 - Min Loss:7.320 - Loss:7.5166355268937415\n",
      " Iter:6 - Alpha:0.047 - Batch 44/274 - Min Loss:7.292 - Loss:7.3182834273581725\n",
      " Iter:6 - Alpha:0.047 - Batch 46/274 - Min Loss:7.260 - Loss:7.272536393527285\n",
      " Iter:6 - Alpha:0.047 - Batch 48/274 - Min Loss:7.255 - Loss:7.266099196801571\n",
      " Iter:6 - Alpha:0.047 - Batch 49/274 - Min Loss:7.255 - Loss:7.255164202448135\n",
      " Iter:6 - Alpha:0.047 - Batch 274/274 - Min Loss:7.254 - Loss:7.4095939119547936\n",
      " Iter:7 - Alpha:0.046 - Batch 1/274 - Min Loss:8.565 - Loss:8.565421653333392 - he whe with meurendus withu mer under the with meurender withe with me\n",
      " Iter:7 - Alpha:0.046 - Batch 2/274 - Min Loss:7.502 - Loss:7.502080356457814\n",
      " Iter:7 - Alpha:0.046 - Batch 34/274 - Min Loss:7.361 - Loss:7.3954568034363035\n",
      " Iter:7 - Alpha:0.046 - Batch 36/274 - Min Loss:7.361 - Loss:7.3714535545285966\n",
      " Iter:7 - Alpha:0.046 - Batch 37/274 - Min Loss:7.328 - Loss:7.328401996326953\n",
      " Iter:7 - Alpha:0.046 - Batch 38/274 - Min Loss:7.322 - Loss:7.322234698377543\n",
      " Iter:7 - Alpha:0.046 - Batch 40/274 - Min Loss:7.313 - Loss:7.334874307206484\n",
      " Iter:7 - Alpha:0.046 - Batch 41/274 - Min Loss:7.299 - Loss:7.299055501704839\n",
      " Iter:7 - Alpha:0.046 - Batch 42/274 - Min Loss:7.294 - Loss:7.29428898732416\n",
      " Iter:7 - Alpha:0.046 - Batch 43/274 - Min Loss:7.293 - Loss:7.293293476012172\n",
      " Iter:7 - Alpha:0.046 - Batch 44/274 - Min Loss:7.284 - Loss:7.284706496423333\n",
      " Iter:7 - Alpha:0.046 - Batch 46/274 - Min Loss:7.227 - Loss:7.240419210552445\n",
      " Iter:7 - Alpha:0.046 - Batch 49/274 - Min Loss:7.223 - Loss:7.225314426078359\n",
      " Iter:7 - Alpha:0.046 - Batch 56/274 - Min Loss:7.220 - Loss:7.2212156469647456\n",
      " Iter:7 - Alpha:0.046 - Batch 57/274 - Min Loss:7.206 - Loss:7.206057904153283\n",
      " Iter:7 - Alpha:0.046 - Batch 58/274 - Min Loss:7.205 - Loss:7.205276887400381\n",
      " Iter:7 - Alpha:0.046 - Batch 60/274 - Min Loss:7.197 - Loss:7.214224985934616\n",
      " Iter:7 - Alpha:0.046 - Batch 61/274 - Min Loss:7.190 - Loss:7.190589252342697\n",
      " Iter:7 - Alpha:0.046 - Batch 274/274 - Min Loss:7.174 - Loss:7.3686536278860655\n",
      " Iter:8 - Alpha:0.046 - Batch 1/274 - Min Loss:8.283 - Loss:8.283628730204015 - he wher withe wher withe wher withus with the urend the wheu ureureure\n",
      " Iter:8 - Alpha:0.046 - Batch 2/274 - Min Loss:7.040 - Loss:7.040162130261881\n",
      " Iter:8 - Alpha:0.046 - Batch 274/274 - Min Loss:6.899 - Loss:7.1942544041645255\n",
      " Iter:9 - Alpha:0.045 - Batch 1/274 - Min Loss:8.154 - Loss:8.154433090390802 - o the und the urese und the with he uruend the wher with und the with \n",
      " Iter:9 - Alpha:0.045 - Batch 2/274 - Min Loss:6.786 - Loss:6.786356465878434\n",
      " Iter:9 - Alpha:0.045 - Batch 274/274 - Min Loss:6.482 - Loss:7.0770459686194955"
     ]
    }
   ],
   "source": [
    "train_2(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_2(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         Whing of my loven as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as asten all as a\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample_2(n=500, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
